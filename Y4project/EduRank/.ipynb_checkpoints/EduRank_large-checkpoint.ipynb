{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3968"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=pd.read_csv(r'D:\\Y4project\\EduRank\\algebra_2005_2006_train.txt',sep='\\t',header=None,engine='python')\n",
    "df=pd.read_csv(r'D:\\Y4project\\EduRank\\algebra_2005_2006_master.txt',sep='\\t',header=None,engine='python')\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.iloc[0]\n",
    "# df2=df.drop(df.index[0])\n",
    "df2=df.drop(df.index[0])\n",
    "train=df2[[\"Problem Name\",\"Incorrects\",\"Step Duration (sec)\"]]\n",
    "question_set=set(df2[\"Problem Name\"])\n",
    "student_set=set(df2[\"Anon Student Id\"])\n",
    "df2[\"Correct First Attempt\"]=pd.to_numeric(df2[\"Correct First Attempt\"])\n",
    "df2[\"Incorrects\"]=pd.to_numeric(df2[\"Incorrects\"])\n",
    "df2[\"Step Duration (sec)\"]=pd.to_numeric(df2[\"Step Duration (sec)\"])\n",
    "difficulty_rankings={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "Duration: 0:00:07.389098\n"
     ]
    }
   ],
   "source": [
    "#difficulty_rankings={}\n",
    "remove_student=set()\n",
    "a=[]\n",
    "\n",
    "for s in student_set:\n",
    "    df_temp=df2[df2[\"Anon Student Id\"]==s]\n",
    "    a.append(len(set(df_temp[\"Problem Name\"])))\n",
    "    if(len(set(df_temp[\"Problem Name\"]))<20):\n",
    "        #df2=df2[df2[\"Anon Student Id\"]!=s]\n",
    "        remove_student.add(s)\n",
    "\n",
    "student_set.difference_update(remove_student)\n",
    "df2=df2[df2[\"Anon Student Id\"].isin(student_set)]\n",
    "question_set=set(df2[\"Problem Name\"])\n",
    "print(len(question_set))\n",
    "\n",
    "# start_time = datetime.now()\n",
    "\n",
    "# def grade(df,qs,ss):\n",
    "#     count=0\n",
    "#     for s in ss:\n",
    "#         #print(count)\n",
    "#         df_temp1=df[df[\"Anon Student Id\"]==s]\n",
    "#         ranking=[]\n",
    "#         for q in qs:\n",
    "#             df_temp2=df_temp1[df_temp1[\"Problem Name\"]==q]\n",
    "#             n=len(df_temp2)\n",
    "#             if(n>0):\n",
    "#                 #First rank based on scores\n",
    "#                 score=df_temp2[\"Correct First Attempt\"].sum()/n\n",
    "#                 #Then tie breaks on incorrect attempts\n",
    "#                 attempts=df_temp2[\"Incorrects\"].sum()/n\n",
    "#                 #Then tie breaks on time taken\n",
    "#                 timetaken=df_temp2[\"Step Duration (sec)\"].sum()/n\n",
    "#                 ranking.append((q,score,attempts,timetaken))\n",
    "#         a=sorted(ranking, key=lambda x:(-x[1],x[2],x[3]))\n",
    "#         difficulty_rankings[s]=[x[0] for x in a]\n",
    "#         count+=1\n",
    "            \n",
    "# grade(df2,question_set,student_set)\n",
    "# end_time = datetime.now()\n",
    "# print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the histogram of number of questions solved vs number of students. We notice that a significant number of students solved less than 20 questions. This poses difficulties to infer the ranking for other questions as the information given is very limited. Hence, we took all such students out of consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([296, 127,  37,  28,  46,  19,  11,   4,   6,   1], dtype=int64), array([ 1. ,  4.5,  8. , 11.5, 15. , 18.5, 22. , 25.5, 29. , 32.5, 36. ]))\n"
     ]
    }
   ],
   "source": [
    "#histogram of number of questions solved vs number of students\n",
    "print(np.histogram(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check whether one question is more difficult than the other\n",
    "def rank(q1,q2,ranking):\n",
    "    if(q1 not in ranking or q2 not in ranking):\n",
    "        return False\n",
    "    #return true if q1 is more difficult than q2\n",
    "    return ranking.index(q1)>ranking.index(q2)\n",
    "\n",
    "#win score over question pairs q1,q2 given a ranking \n",
    "def gamma(q1,q2,ranking):\n",
    "    if(q1 not in ranking or q2 not in ranking):\n",
    "        return 0\n",
    "    return 1 if rank(q1,q2,ranking) else -1\n",
    "\n",
    "#all question pairs in L such that qj>qk\n",
    "def Zk(L,ranking,k):\n",
    "    answer=[]\n",
    "    for q in L:\n",
    "        if(rank(q,k,ranking)):\n",
    "            answer.append((q,k))\n",
    "    return answer\n",
    "\n",
    "#Indicator function\n",
    "def IA(ranking1,ranking2,q1,q2):\n",
    "    return 1 if (rank(q1,q2,ranking1) and rank(q1,q2,ranking2)) else 0\n",
    "\n",
    "#normalized agreement score between two rankings\n",
    "def Ak(L,ranking1,ranking2,k):\n",
    "    Z=Zk(L,ranking2,k)\n",
    "    if(len(Z)==0):\n",
    "        return 0\n",
    "    answer=0\n",
    "    for (q1,q2) in Z:\n",
    "        answer+=IA(ranking1,ranking2,q1,q2)\n",
    "    return answer/(ranking2.index(k))\n",
    "\n",
    "#AP score of ranking2 over L\n",
    "def SAP(L,ranking1,ranking2):\n",
    "    answer=0\n",
    "    n=len(L)\n",
    "    for i in range(1,n):\n",
    "        answer+=Ak(L,ranking1,ranking2,L[i])\n",
    "    return answer/(n-1)\n",
    "\n",
    "#relative voting of two questions\n",
    "def rv(q1,q2,S,si,Ti):\n",
    "    a=0\n",
    "    for j in S:\n",
    "        if(j!=i):\n",
    "            a+=SAP(Ti,difficulty_rankings[i],difficulty_rankings[j])*gamma(q1,q2,difficulty_rankings[j])\n",
    "    return np.sign(a)\n",
    "\n",
    "#Copeland score\n",
    "def c(q,S,si,Li,Ti):\n",
    "    answer=0\n",
    "    for ql in Li:\n",
    "        if (ql!=q):\n",
    "            answer+=rv(q,ql,S,si,Ti)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EG-EPS02',\n",
       " 'LIT32A',\n",
       " '7X-4YGE15',\n",
       " 'EG-S-FACTOR16',\n",
       " 'LIT6A',\n",
       " 'PROP10',\n",
       " 'DIST04_SP',\n",
       " 'NEWQUAD7TRE',\n",
       " 'BH1T45',\n",
       " 'EG61',\n",
       " 'EG41',\n",
       " 'NEWSQUARE5TRE']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EduRank Algorithm\n",
    "def EduRank(si,Ti,Li):\n",
    "    #questions to be ranked\n",
    "    dict=[]\n",
    "    for q in Li:\n",
    "        dict.append((q,c(q,student_set,i,Li,Ti)))\n",
    "        print(len(dict))\n",
    "    dict=sorted(dict, key=lambda x:-x[1])\n",
    "    return dict\n",
    "\n",
    "#split questions into train and test sets\n",
    "def split_questions(si):\n",
    "    df3=df2[df2[\"Anon Student Id\"]==si]\n",
    "    qq=list(set(df3[\"Problem Name\"]))\n",
    "    n=int(len(qq)/2)\n",
    "    return qq[0:n],qq[n:len(qq)],df3\n",
    "\n",
    "#infer the difficulty ranking for a student\n",
    "def rank_questions(df,train,test);\n",
    "    #print(count)\n",
    "    n=df.shape[0]\n",
    "    ranking_train,ranking_test=[]\n",
    "    for q in train:       \n",
    "        #First rank based on scores\n",
    "        score=df[\"Correct First Attempt\"].sum()/n\n",
    "        #Then tie breaks on incorrect attempts\n",
    "        attempts=df[\"Incorrects\"].sum()/n\n",
    "        #Then tie breaks on time taken\n",
    "        timetaken=df[\"Step Duration (sec)\"].sum()/n\n",
    "        ranking_train.append((q,score,attempts,timetaken))\n",
    "    for q in test:       \n",
    "        score=df[\"Correct First Attempt\"].sum()/n\n",
    "        attempts=df[\"Incorrects\"].sum()/n\n",
    "        timetaken=df[\"Step Duration (sec)\"].sum()/n\n",
    "        ranking_test.append((q,score,attempts,timetaken))\n",
    "    a=sorted(ranking_train, key=lambda x:(x[1],-x[2],-x[3]))\n",
    "    b=sorted(ranking_test, key=lambda x:(x[1],-x[2],-x[3]))\n",
    "    return [x[0] for x in a],[x[0] for x in b]\n",
    "\n",
    "# qq=select_questions('1bJbgQ32E3')\n",
    "# qq\n",
    "\n",
    "start_time = datetime.now()\n",
    "train,test=split_questions('1bJbgQ32E3')\n",
    "EduRank('1bJbgQ32E3',train,test)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EG-EPS02',\n",
       " 'LIT32A',\n",
       " '7X-4YGE15',\n",
       " 'EG-S-FACTOR16',\n",
       " 'LIT6A',\n",
       " 'PROP10',\n",
       " 'DIST04_SP',\n",
       " 'NEWQUAD7TRE',\n",
       " 'BH1T45',\n",
       " 'EG61',\n",
       " 'EG41',\n",
       " 'NEWSQUARE5TRE',\n",
       " 'EG25',\n",
       " 'FEB13',\n",
       " 'L5FB02',\n",
       " 'LIT70A',\n",
       " 'REAL28',\n",
       " 'EG-FACTOR50',\n",
       " 'SYSFB10',\n",
       " 'SYS02',\n",
       " 'EG-EQS07',\n",
       " '2PTFB10',\n",
       " 'BH1T32B',\n",
       " 'BH1T33B']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difficulty_rankings['1bJbgQ32E3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-b1c5cd67aa60>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-b1c5cd67aa60>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    a[0;2]\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a=set([1,2,3,4])\n",
    "a[0;2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
